# W&B Sweep Configuration for Dendritic Optimization
# This sweep tunes both model compression and PerforatedAI hyperparameters

program: train_dendritic_summarizer.py
method: bayes
metric:
  name: val_rouge_l
  goal: maximize

parameters:
  # Training hyperparameters
  batch_size:
    values: [8, 16, 32]
  lr:
    distribution: uniform
    min: 1e-6
    max: 5e-4
  weight_decay:
    values: [0.0, 1e-6, 1e-4]
  max_steps:
    values: [2000, 5000, 10000]
  
  # Model selection
  model_type:
    values: ["t5-small", "t5-base", "bart-large-cnn"]
  
  # Compression architecture (encoder)
  enc_num_layers:
    values: [4, 6, 8]
  enc_hidden_size:
    values: [256, 384, 512]
  enc_num_heads:
    values: [4, 6, 8]

  # Compression architecture (decoder for T5/BART)
  dec_num_layers:
    values: [3, 4, 6]
  dec_d_model:
    values: [256, 384, 512]

  # PerforatedAI (PAI) dendritic optimization
  do_pb:
    values: [true, false]
  N_EPOCHS_TO_SWITCH:
    values: [3, 5, 10]
  P_EPOCHS_TO_SWITCH:
    values: [1, 3, 5]
  CAP_N:
    values: [true, false]
  TEST_DENDRITE_CAPACITY:
    values: [true, false]
  
  # Dataset configuration
  dataset:
    values: ["reddit_tldr", "samsum", "cnn_dailymail", "xsum"]
  
  # Early stopping
  patience:
    values: [5, 10, 15]
  
  # Random seed
  seed:
    values: [0, 42, 1234]
