export const MOCK_SUMMARIES: Record<string, any> = {
  demo: {
    tokens: [
      "The ",
      "thread ",
      "discusses ",
      "best ",
      "practices ",
      "for ",
      "streaming ",
      "LLM ",
      "responses ",
      "when ",
      "summarizing ",
      "long ",
      "forum ",
      "threads. ",
      "\n\n",
      "Key ",
      "recommendations ",
      "include: ",
      "chunking ",
      "text ",
      "into ",
      "~512 ",
      "token ",
      "windows, ",
      "caching ",
      "embeddings ",
      "per-sentence ",
      "to ",
      "avoid ",
      "recomputation, ",
      "and ",
      "using ",
      "a ",
      "two-pass ",
      "approach ",
      "with ",
      "a ",
      "fast ",
      "draft ",
      "model ",
      "followed ",
      "by ",
      "quality ",
      "consolidation. ",
      "\n\n",
      "Important ",
      "considerations: ",
      "strip ",
      "or ",
      "handle ",
      "code ",
      "blocks ",
      "separately ",
      "to ",
      "prevent ",
      "token ",
      "explosion, ",
      "maintain ",
      "character-level ",
      "provenance ",
      "tracking ",
      "for ",
      "verification, ",
      "and ",
      "optimize ",
      "for ",
      "sub-200ms ",
      "first ",
      "token ",
      "latency.",
    ],
    final_summary:
      "The thread discusses best practices for streaming LLM responses when summarizing long forum threads. Key recommendations include chunking text into ~512 token windows, caching embeddings per-sentence to avoid recomputation, and using a two-pass approach with a fast draft model followed by quality consolidation. Important considerations include stripping code blocks to prevent token explosion, maintaining character-level provenance tracking for verification, and optimizing for sub-200ms first token latency.",
    digest: [
      "Chunk text into ~512 token windows for efficient processing",
      "Cache embeddings per-sentence to avoid recomputation overhead",
      "Use two-pass approach: fast streaming draft + quality consolidation",
      "Strip or handle code blocks separately to prevent token explosion",
      "Maintain character offsets for provenance tracking and verification",
      "Verdict: Two-pass streaming with caching achieves 40% latency reduction while maintaining accuracy",
    ],
    highlights: [
      {
        id: "hl_0",
        text: "You can chunk text into windows of ~512 tokens and stream partial summaries.",
        category: "solution",
        confidence: 0.92,
        importance_score: 0.88,
        why_matters: "Core technical solution addressing the original question about async streaming",
        post_id: "p2",
        author_id: "bob",
        post_position: 1,
        sentence_index: 0,
        start_offset: 0,
        end_offset: 79,
      },
      {
        id: "hl_1",
        text: "Cache embeddings per-sentence to avoid recompute.",
        category: "solution",
        confidence: 0.89,
        importance_score: 0.85,
        why_matters: "Performance optimization technique that prevents redundant computation",
        post_id: "p2",
        author_id: "bob",
        post_position: 1,
        sentence_index: 1,
        start_offset: 80,
        end_offset: 130,
      },
      {
        id: "hl_2",
        text: "Using a shorter draft model for streaming helped reduce latency to under 200ms for first token.",
        category: "solution",
        confidence: 0.87,
        importance_score: 0.82,
        why_matters: "Concrete performance metric demonstrating practical latency improvements",
        post_id: "p3",
        author_id: "carmen",
        post_position: 2,
        sentence_index: 2,
        start_offset: 120,
        end_offset: 216,
      },
      {
        id: "hl_3",
        text: "This lets you link highlights back to exact locations in posts.",
        category: "fact",
        confidence: 0.91,
        importance_score: 0.79,
        why_matters: "Essential feature for verification and provenance tracking",
        post_id: "p4",
        author_id: "dan",
        post_position: 3,
        sentence_index: 1,
        start_offset: 85,
        end_offset: 149,
      },
      {
        id: "hl_4",
        text: "Cut total latency by 40% while maintaining accuracy.",
        category: "fact",
        confidence: 0.94,
        importance_score: 0.91,
        why_matters: "Quantified improvement showing significant performance gains without quality loss",
        post_id: "p5",
        author_id: "eve",
        post_position: 4,
        sentence_index: 2,
        start_offset: 140,
        end_offset: 193,
      },
    ],
  },
  "ai-optimization": {
    tokens: [
      "This ",
      "thread ",
      "explores ",
      "dendritic ",
      "optimization ",
      "techniques ",
      "for ",
      "neural ",
      "networks ",
      "using ",
      "the ",
      "perforatedai ",
      "library. ",
      "\n\n",
      "The ",
      "primary ",
      "finding ",
      "is ",
      "achieving ",
      "40% ",
      "parameter ",
      "reduction ",
      "with ",
      "minimal ",
      "accuracy ",
      "loss ",
      "through ",
      "perforated ",
      "backpropagation. ",
      "\n\n",
      "The ",
      "approach ",
      "works ",
      "with ",
      "transformers ",
      "including ",
      "T5-small ",
      "for ",
      "summarization, ",
      "using ",
      "W&B ",
      "sweeps ",
      "for ",
      "hyperparameter ",
      "tuning ",
      "and ",
      "proper ",
      "attention ",
      "layer ",
      "wrapping.",
    ],
    final_summary:
      "This thread explores dendritic optimization techniques for neural networks using the perforatedai library. The primary finding is achieving 40% parameter reduction with minimal accuracy loss through perforated backpropagation. The approach works with transformers including T5-small for summarization, using W&B sweeps for hyperparameter tuning and proper attention layer wrapping.",
    digest: [
      "Dendritic optimization achieves 40% parameter reduction with minimal accuracy loss",
      "Uses perforatedai library with perforated backpropagation tracker",
      "Works with transformers including T5-small for summarization tasks",
      "W&B sweeps recommended for hyperparameter tuning",
      "Key requirement: proper wrapping of attention layers",
      "Verdict: More principled than sparse attention patterns for model compression",
    ],
    highlights: [
      {
        id: "hl_0",
        text: "Initial results show 40% parameter reduction with minimal accuracy loss.",
        category: "fact",
        confidence: 0.95,
        importance_score: 0.93,
        why_matters: "Primary quantified result demonstrating effectiveness of the approach",
        post_id: "p1",
        author_id: "researcher1",
        post_position: 0,
        sentence_index: 1,
        start_offset: 85,
        end_offset: 157,
      },
      {
        id: "hl_1",
        text: "Using W&B sweeps for hyperparameter tuning.",
        category: "solution",
        confidence: 0.88,
        importance_score: 0.82,
        why_matters: "Practical implementation detail for reproducible research",
        post_id: "p3",
        author_id: "researcher1",
        post_position: 2,
        sentence_index: 0,
        start_offset: 0,
        end_offset: 43,
      },
      {
        id: "hl_2",
        text: "The key is the perforated backpropagation trackerâ€”it automatically prunes low-impact connections during training.",
        category: "solution",
        confidence: 0.92,
        importance_score: 0.89,
        why_matters: "Core mechanism explaining how the optimization works",
        post_id: "p3",
        author_id: "researcher1",
        post_position: 2,
        sentence_index: 1,
        start_offset: 44,
        end_offset: 157,
      },
      {
        id: "hl_3",
        text: "Works great with transformers!",
        category: "fact",
        confidence: 0.86,
        importance_score: 0.78,
        why_matters: "Confirms broader applicability beyond just CNNs",
        post_id: "p5",
        author_id: "researcher1",
        post_position: 4,
        sentence_index: 0,
        start_offset: 0,
        end_offset: 30,
      },
      {
        id: "hl_4",
        text: "The key is wrapping attention layers properly.",
        category: "solution",
        confidence: 0.9,
        importance_score: 0.84,
        why_matters: "Critical implementation detail for transformer models",
        post_id: "p5",
        author_id: "researcher1",
        post_position: 4,
        sentence_index: 2,
        start_offset: 80,
        end_offset: 126,
      },
    ],
  },
  "react-patterns": {
    tokens: [
      "The ",
      "thread ",
      "addresses ",
      "state ",
      "management ",
      "patterns ",
      "for ",
      "complex ",
      "multi-step ",
      "forms ",
      "in ",
      "React. ",
      "\n\n",
      "The ",
      "consensus ",
      "recommendation ",
      "is ",
      "to ",
      "use ",
      "React ",
      "Hook ",
      "Form ",
      "with ",
      "Zod ",
      "validation ",
      "for ",
      "90% ",
      "of ",
      "use ",
      "cases, ",
      "as ",
      "it ",
      "handles ",
      "form ",
      "state ",
      "management ",
      "out ",
      "of ",
      "the ",
      "box ",
      "with ",
      "excellent ",
      "TypeScript ",
      "support. ",
      "\n\n",
      "For ",
      "custom ",
      "requirements, ",
      "the ",
      "alternative ",
      "is ",
      "using ",
      "useReducer ",
      "for ",
      "complex ",
      "state ",
      "logic ",
      "combined ",
      "with ",
      "useContext ",
      "for ",
      "sharing ",
      "state, ",
      "keeping ",
      "validation ",
      "logic ",
      "separate ",
      "in ",
      "custom ",
      "hooks.",
    ],
    final_summary:
      "The thread addresses state management patterns for complex multi-step forms in React. The consensus recommendation is to use React Hook Form with Zod validation for 90% of use cases, as it handles form state management with excellent TypeScript support. For custom requirements, the alternative is using useReducer for complex state logic combined with useContext for sharing state, keeping validation logic separate in custom hooks.",
    digest: [
      "React Hook Form with Zod validation recommended for 90% of use cases",
      "Provides built-in form state management with TypeScript support",
      "Alternative: useReducer for complex state, useContext for sharing",
      "Keep validation logic separate in custom hooks (useFormValidation)",
      "Separation of concerns: reducer for state, hooks for business logic",
      "Verdict: Start with React Hook Form for quick wins, fall back to reducer pattern for custom needs",
    ],
    highlights: [
      {
        id: "hl_0",
        text: "Use useReducer for complex state logic.",
        category: "solution",
        confidence: 0.91,
        importance_score: 0.87,
        why_matters: "Core recommendation for managing complex form state predictably",
        post_id: "p2",
        author_id: "react_expert",
        post_position: 1,
        sentence_index: 0,
        start_offset: 0,
        end_offset: 40,
      },
      {
        id: "hl_1",
        text: "Combine with useContext for sharing state across components.",
        category: "solution",
        confidence: 0.89,
        importance_score: 0.83,
        why_matters: "Explains how to share reducer state across component tree",
        post_id: "p2",
        author_id: "react_expert",
        post_position: 1,
        sentence_index: 2,
        start_offset: 120,
        end_offset: 180,
      },
      {
        id: "hl_2",
        text: "Keep validation logic separate in custom hooks like useFormValidation.",
        category: "solution",
        confidence: 0.9,
        importance_score: 0.85,
        why_matters: "Architectural pattern for clean separation of concerns",
        post_id: "p4",
        author_id: "react_expert",
        post_position: 3,
        sentence_index: 0,
        start_offset: 0,
        end_offset: 70,
      },
      {
        id: "hl_3",
        text: "Or just use React Hook Form with Zod validation.",
        category: "solution",
        confidence: 0.94,
        importance_score: 0.92,
        why_matters: "Highly upvoted pragmatic solution that handles 90% of use cases",
        post_id: "p5",
        author_id: "another_dev",
        post_position: 4,
        sentence_index: 0,
        start_offset: 0,
        end_offset: 49,
      },
      {
        id: "hl_4",
        text: "Handles 90% of form state management out of the box and has great TypeScript support.",
        category: "fact",
        confidence: 0.93,
        importance_score: 0.89,
        why_matters: "Quantified benefit showing broad applicability",
        post_id: "p5",
        author_id: "another_dev",
        post_position: 4,
        sentence_index: 1,
        start_offset: 50,
        end_offset: 135,
      },
    ],
  },
}

export const MOCK_INSIGHTS: Record<string, any> = {
  demo: {
    sentiment: {
      overall: "positive",
      distribution: { positive: 70, neutral: 25, negative: 5 },
      trend: "improving",
      notable_shifts: [
        {
          post_position: 4,
          sentiment: "very positive",
          reason: "Eve shares successful implementation with quantified results",
        },
      ],
    },
    topics: [
      { name: "Streaming Performance", relevance: 0.95, posts: [0, 1, 2, 4] },
      { name: "Caching Strategies", relevance: 0.88, posts: [1, 2] },
      { name: "Provenance Tracking", relevance: 0.82, posts: [3, 4] },
      { name: "Two-Pass Architecture", relevance: 0.91, posts: [1, 4] },
    ],
    expertise: [
      {
        user_id: "eve",
        username: "Eve Thompson",
        expertise_score: 0.94,
        contributions: "Implemented successful two-pass approach with 40% latency reduction",
        recommended_for: "Architecture design, Performance optimization",
      },
      {
        user_id: "bob",
        username: "Bob Smith",
        expertise_score: 0.89,
        contributions: "Detailed technical solutions for chunking and caching",
        recommended_for: "Implementation details, Caching strategies",
      },
      {
        user_id: "dan",
        username: "Dan Lee",
        expertise_score: 0.85,
        contributions: "Provenance tracking with character offsets",
        recommended_for: "Data integrity, Verification systems",
      },
    ],
    smart_replies: [
      {
        context: "Follow-up question about implementation",
        suggested_reply:
          "Based on Eve's experience, consider implementing the two-pass approach: start with a fast streaming model (GPT-4o-mini) for sub-200ms first token, then consolidate with a quality pass in parallel. Remember to cache embeddings per-sentence as Bob suggested.",
      },
      {
        context: "Question about code handling",
        suggested_reply:
          "As Carmen mentioned, code blocks cause token explosion. You can either strip them entirely or send them as attachments. Consider detecting code blocks with regex (```...```) and replacing with a placeholder like [CODE_BLOCK_1].",
      },
    ],
    trends: [
      {
        pattern: "Two-pass architecture consensus",
        confidence: 0.92,
        implications:
          "Community converging on fast draft + quality consolidation pattern for optimal latency/accuracy tradeoff",
      },
      {
        pattern: "Performance-first approach",
        confidence: 0.88,
        implications: "Sub-200ms first token becoming the expected standard for streaming summaries",
      },
    ],
  },
  "ai-optimization": {
    sentiment: {
      overall: "positive",
      distribution: { positive: 75, neutral: 20, negative: 5 },
      trend: "stable",
      notable_shifts: [
        {
          post_position: 2,
          sentiment: "enthusiastic",
          reason: "Researcher offers to share configuration, fostering collaboration",
        },
      ],
    },
    topics: [
      { name: "Dendritic Optimization", relevance: 0.98, posts: [0, 1, 2, 4] },
      { name: "Parameter Reduction", relevance: 0.95, posts: [0, 1] },
      { name: "Transformer Compression", relevance: 0.89, posts: [3, 4] },
      { name: "Training Configuration", relevance: 0.86, posts: [2, 4] },
    ],
    expertise: [
      {
        user_id: "researcher1",
        username: "Dr. Sarah Johnson",
        expertise_score: 0.96,
        contributions: "40% parameter reduction results, T5-small implementation, W&B sweeps",
        recommended_for: "Research methodology, Model compression, Training optimization",
      },
      {
        user_id: "mleng",
        username: "Mike Chen",
        expertise_score: 0.82,
        contributions: "Experience with sparse attention patterns for comparison",
        recommended_for: "Alternative approaches, Performance benchmarking",
      },
    ],
    smart_replies: [
      {
        context: "Request for configuration details",
        suggested_reply:
          "Dr. Johnson mentioned using W&B sweeps with the perforated backpropagation tracker. Key is proper wrapping of attention layers for transformers. Check the PAI module wrapper documentation for transformer-specific implementations.",
      },
      {
        context: "Question about applicability to other models",
        suggested_reply:
          "Dr. Johnson confirmed it works with transformers (T5-small tested). For BERT compression, you'll need to wrap the attention layers properly. The approach is more principled than sparse attention patterns according to Mike's observation.",
      },
    ],
    trends: [
      {
        pattern: "Growing interest in dendritic optimization",
        confidence: 0.91,
        implications: "Potential shift from traditional pruning to biologically-inspired optimization methods",
      },
      {
        pattern: "Academic-practitioner collaboration",
        confidence: 0.87,
        implications:
          "Research findings being actively applied to production use cases (thesis work, real implementations)",
      },
    ],
  },
  "react-patterns": {
    sentiment: {
      overall: "helpful",
      distribution: { positive: 80, neutral: 18, negative: 2 },
      trend: "very positive",
      notable_shifts: [
        {
          post_position: 4,
          sentiment: "very positive",
          reason: "Highly upvoted pragmatic solution (React Hook Form) gets strong community support",
        },
      ],
    },
    topics: [
      { name: "Form State Management", relevance: 0.97, posts: [0, 1, 4, 5] },
      { name: "useReducer Pattern", relevance: 0.91, posts: [1, 3] },
      { name: "Form Validation", relevance: 0.88, posts: [2, 3] },
      { name: "React Hook Form", relevance: 0.95, posts: [4, 5] },
    ],
    expertise: [
      {
        user_id: "react_expert",
        username: "Sam Wilson",
        expertise_score: 0.92,
        contributions: "Detailed guidance on useReducer pattern and validation separation",
        recommended_for: "React patterns, State management architecture",
      },
      {
        user_id: "another_dev",
        username: "Casey Brown",
        expertise_score: 0.94,
        contributions: "Pragmatic recommendation (React Hook Form) with highest community approval",
        recommended_for: "Practical solutions, Library recommendations",
      },
    ],
    smart_replies: [
      {
        context: "Choosing between approaches",
        suggested_reply:
          "Start with React Hook Form + Zod as Casey suggested (handles 90% of cases with great TypeScript support). If you need highly custom logic, fall back to Sam's reducer pattern with separate validation hooks. The community clearly favors the pragmatic approach first.",
      },
      {
        context: "Implementation details for reducer",
        suggested_reply:
          "Sam recommends: useReducer for state transitions, useContext for sharing across components, and separate custom hooks (like useFormValidation) for business logic. This keeps a clean separation of concerns.",
      },
    ],
    trends: [
      {
        pattern: "Library-first approach gaining favor",
        confidence: 0.93,
        implications: "Community preferring battle-tested libraries (React Hook Form) over custom implementations",
      },
      {
        pattern: "TypeScript integration as key criterion",
        confidence: 0.89,
        implications: "Type safety becoming a primary factor in library selection and architecture decisions",
      },
    ],
  },
}

export function getMockSummaryData(threadId: string) {
  return MOCK_SUMMARIES[threadId] || null
}

export function getMockInsights(threadId: string) {
  return MOCK_INSIGHTS[threadId] || null
}

// Simulate streaming with realistic timing
export async function* simulateTokenStream(tokens: string[], delayMs = 30) {
  for (const token of tokens) {
    await new Promise((resolve) => setTimeout(resolve, delayMs))
    yield token
  }
}
